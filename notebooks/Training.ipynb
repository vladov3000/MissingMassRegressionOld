{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLNuDataset(torch.utils.data.Dataset):\n",
    "     def __init__(self, df_path=\"../data/gen.csv\"):\n",
    "        df = pd.read_csv(df_path)\n",
    "        df = df.drop(labels=[\"W_px\", \"W_py\", \"W_pz\", \"W_E\", \"L_E\", \"Nu_E\"], axis=1)\n",
    "        \n",
    "        # standardize input columns\n",
    "        x = df.loc[:, df.columns != \"Nu_pz\"].to_numpy()\n",
    "        scaler = preprocessing.StandardScaler().fit(x)\n",
    "        x = scaler.transform(x)\n",
    "        df[df.columns[:-1]] = x\n",
    "        \n",
    "        self.data = torch.from_numpy(df.values).float()\n",
    " \n",
    "     def __getitem__(self, idx):\n",
    "         return self.data[idx]\n",
    " \n",
    "     def __len__(self):\n",
    "         return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLNuModel(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_fn):\n",
    "    \n",
    "    n_features = model.n_features\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        X = batch[:, :n_features]\n",
    "        Y = batch[:, n_features:]\n",
    "\n",
    "        out = model(X)\n",
    "        loss = loss_fn(out, Y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_split = [0.9, 0.05, 0.05], batch_size=8192, n_epochs=100, learning_rate=1e-3):\n",
    "    dataset = WLNuDataset()\n",
    "    n_features = len(dataset[0]) - 1 # last two columns are targets: N_z\n",
    "    \n",
    "    print(f\"# of input features: {n_features}\")\n",
    "    print(f\"Total number of samples: {len(dataset)}\")\n",
    "    \n",
    "    data_split = [int(i * len(dataset)) for i in data_split]\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(dataset, lengths=data_split)\n",
    "    \n",
    "    print(f\"Train set samples: {len(train_set)}\")\n",
    "    print(f\"Val set samples: {len(val_set)}\")\n",
    "    print(f\"Test set samples: {len(test_set)}\")\n",
    "    \n",
    "    train_loader, val_loader, test_loader = [DataLoader(i, batch_size=batch_size, shuffle=True) \n",
    "                                             for i in (train_set, val_set, test_set)]\n",
    "    \n",
    "    model = WLNuModel(n_features)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f\"\\n{'='*30}\\n\")\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print(f\"Epoch {epoch}\")\n",
    "        \n",
    "        train_loss = 0\n",
    "        \n",
    "        for train_batch in train_loader:\n",
    "            X = train_batch[:, :n_features]\n",
    "            Y = train_batch[:, n_features:]\n",
    "            \n",
    "            out = model(X)\n",
    "            loss = loss_fn(out, Y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad() # clear previous gradients\n",
    "            loss.backward() # compute gradients\n",
    "            \n",
    "            optimizer.step() # update weights using computed gradients\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Train loss: {train_loss}\")\n",
    "        \n",
    "        val_loss = evaluate(model, val_loader, loss_fn)\n",
    "        print(f\"Val loss: {val_loss}\")\n",
    "        \n",
    "        print(f\"\\n{'='*30}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input features: 5\n",
      "Total number of samples: 100000\n",
      "Train set samples: 90000\n",
      "Val set samples: 5000\n",
      "Test set samples: 5000\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 0\n",
      "Train loss: 1167.9853404651988\n",
      "Val loss: 1096.760498046875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 1\n",
      "Train loss: 1164.7868208451705\n",
      "Val loss: 1091.45556640625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 2\n",
      "Train loss: 1151.069491299716\n",
      "Val loss: 1076.5557861328125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 3\n",
      "Train loss: 1129.7309681285512\n",
      "Val loss: 1054.3587646484375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 4\n",
      "Train loss: 1097.895419034091\n",
      "Val loss: 1016.8220825195312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 5\n",
      "Train loss: 1042.818076393821\n",
      "Val loss: 951.0092163085938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 6\n",
      "Train loss: 978.0543157404119\n",
      "Val loss: 885.7515258789062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 7\n",
      "Train loss: 931.6553455699574\n",
      "Val loss: 848.2990112304688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 8\n",
      "Train loss: 893.9603493430398\n",
      "Val loss: 810.2698974609375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 9\n",
      "Train loss: 854.3333573774858\n",
      "Val loss: 782.2012939453125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 10\n",
      "Train loss: 811.3327747691761\n",
      "Val loss: 748.4339599609375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 11\n",
      "Train loss: 778.6949240944602\n",
      "Val loss: 725.5910034179688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 12\n",
      "Train loss: 743.7549549449574\n",
      "Val loss: 697.0639038085938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 13\n",
      "Train loss: 717.4516157670455\n",
      "Val loss: 671.745849609375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 14\n",
      "Train loss: 696.7561590021306\n",
      "Val loss: 650.9561157226562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 15\n",
      "Train loss: 678.8229703036221\n",
      "Val loss: 634.1035766601562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 16\n",
      "Train loss: 663.968428178267\n",
      "Val loss: 621.4683227539062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 17\n",
      "Train loss: 651.7315451882102\n",
      "Val loss: 613.09912109375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 18\n",
      "Train loss: 643.0868141867898\n",
      "Val loss: 604.8548583984375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 19\n",
      "Train loss: 635.6710704456676\n",
      "Val loss: 598.7551879882812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 20\n",
      "Train loss: 631.1835604580966\n",
      "Val loss: 594.9467163085938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 21\n",
      "Train loss: 625.6961059570312\n",
      "Val loss: 589.9463500976562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 22\n",
      "Train loss: 620.5855934836648\n",
      "Val loss: 588.837158203125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 23\n",
      "Train loss: 620.9527643377131\n",
      "Val loss: 582.4591674804688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 24\n",
      "Train loss: 615.9379272460938\n",
      "Val loss: 579.9093627929688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 25\n",
      "Train loss: 612.178993918679\n",
      "Val loss: 578.017578125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 26\n",
      "Train loss: 607.9436534534801\n",
      "Val loss: 580.2986450195312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 27\n",
      "Train loss: 605.0355613014915\n",
      "Val loss: 571.5283813476562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 28\n",
      "Train loss: 599.7839910333806\n",
      "Val loss: 574.0602416992188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 29\n",
      "Train loss: 598.2642211914062\n",
      "Val loss: 565.46923828125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 30\n",
      "Train loss: 595.479658647017\n",
      "Val loss: 569.5557250976562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 31\n",
      "Train loss: 597.921425559304\n",
      "Val loss: 565.8572998046875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 32\n",
      "Train loss: 595.5939663973721\n",
      "Val loss: 570.1273193359375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 33\n",
      "Train loss: 592.5542491566051\n",
      "Val loss: 562.68310546875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 34\n",
      "Train loss: 586.4698486328125\n",
      "Val loss: 561.5051879882812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 35\n",
      "Train loss: 585.8659556995739\n",
      "Val loss: 560.5916137695312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 36\n",
      "Train loss: 586.1991743607955\n",
      "Val loss: 557.7095336914062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 37\n",
      "Train loss: 585.0669444691051\n",
      "Val loss: 557.2409057617188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 38\n",
      "Train loss: 585.1545743075284\n",
      "Val loss: 554.4158325195312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 39\n",
      "Train loss: 579.922685102983\n",
      "Val loss: 553.6798706054688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 40\n",
      "Train loss: 578.9802135120739\n",
      "Val loss: 558.1299438476562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 41\n",
      "Train loss: 576.7536066228694\n",
      "Val loss: 551.75830078125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 42\n",
      "Train loss: 575.3286854137074\n",
      "Val loss: 549.9422607421875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 43\n",
      "Train loss: 574.1934315074574\n",
      "Val loss: 547.5396118164062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 44\n",
      "Train loss: 573.1943525834517\n",
      "Val loss: 546.7152709960938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 45\n",
      "Train loss: 571.2320501154119\n",
      "Val loss: 546.86767578125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 46\n",
      "Train loss: 570.3975219726562\n",
      "Val loss: 544.7990112304688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 47\n",
      "Train loss: 571.3830233487216\n",
      "Val loss: 544.2937622070312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 48\n",
      "Train loss: 569.9919766512784\n",
      "Val loss: 547.618896484375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 49\n",
      "Train loss: 568.1772128018466\n",
      "Val loss: 544.0789794921875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 50\n",
      "Train loss: 569.6595458984375\n",
      "Val loss: 547.5742797851562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 51\n",
      "Train loss: 569.69677734375\n",
      "Val loss: 546.6874389648438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 52\n",
      "Train loss: 567.4919267134233\n",
      "Val loss: 544.8074340820312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 53\n",
      "Train loss: 564.1917835582386\n",
      "Val loss: 540.2490234375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 54\n",
      "Train loss: 562.6681629527699\n",
      "Val loss: 541.8015747070312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 55\n",
      "Train loss: 560.3470458984375\n",
      "Val loss: 542.7797241210938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 56\n",
      "Train loss: 562.4522705078125\n",
      "Val loss: 542.39501953125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 57\n",
      "Train loss: 558.7978016246449\n",
      "Val loss: 537.9155883789062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 58\n",
      "Train loss: 557.918740012429\n",
      "Val loss: 537.6166381835938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 59\n",
      "Train loss: 557.4089854847301\n",
      "Val loss: 537.6624755859375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 60\n",
      "Train loss: 559.0680486505681\n",
      "Val loss: 538.1175537109375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 61\n",
      "Train loss: 559.4639004794034\n",
      "Val loss: 541.1785278320312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 62\n",
      "Train loss: 560.290488503196\n",
      "Val loss: 538.4146728515625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 63\n",
      "Train loss: 556.3074174360795\n",
      "Val loss: 534.7743530273438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 64\n",
      "Train loss: 554.5733032226562\n",
      "Val loss: 535.62646484375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 65\n",
      "Train loss: 552.7959872159091\n",
      "Val loss: 533.511474609375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 66\n",
      "Train loss: 553.3545809659091\n",
      "Val loss: 533.6425170898438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 67\n",
      "Train loss: 553.4906283291904\n",
      "Val loss: 534.9668579101562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 68\n",
      "Train loss: 551.6583473899148\n",
      "Val loss: 534.1511840820312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 69\n",
      "Train loss: 553.5809936523438\n",
      "Val loss: 533.1295776367188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 70\n",
      "Train loss: 551.183876731179\n",
      "Val loss: 531.511474609375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 71\n",
      "Train loss: 550.8131325461648\n",
      "Val loss: 530.5779418945312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 72\n",
      "Train loss: 549.5748457475142\n",
      "Val loss: 529.6362915039062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 73\n",
      "Train loss: 548.3627430308949\n",
      "Val loss: 530.8128051757812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 74\n",
      "Train loss: 548.6294666637074\n",
      "Val loss: 532.4009399414062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 75\n",
      "Train loss: 548.9515491832386\n",
      "Val loss: 529.8485717773438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 76\n",
      "Train loss: 548.2596712979404\n",
      "Val loss: 532.1664428710938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 77\n",
      "Train loss: 548.1200727982955\n",
      "Val loss: 530.5072631835938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 78\n",
      "Train loss: 547.3619773171165\n",
      "Val loss: 529.9541015625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 79\n",
      "Train loss: 547.7143110795455\n",
      "Val loss: 530.1752319335938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 80\n",
      "Train loss: 545.7704412286931\n",
      "Val loss: 529.1683959960938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 545.2250310724431\n",
      "Val loss: 528.0106201171875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-253-3541cdf21573>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_split, batch_size, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/missing-mass-env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/missing-mass-env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/missing-mass-env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/missing-mass-env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/missing-mass-env/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-250-c7fd4d4fcc37>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m      \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m          \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m      \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
