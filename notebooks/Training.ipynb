{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLNuDataset(torch.utils.data.Dataset):\n",
    "     def __init__(self, df_path=\"../wlnu/data/gen.csv\"):\n",
    "        df = pd.read_csv(df_path)\n",
    "        df = df.drop(labels=[\"W_px\", \"W_py\", \"W_pz\", \"W_m\", \"L_E\", \"Nu_E\"], axis=1)\n",
    "        \n",
    "        # standardize input columns\n",
    "        # x = df.loc[:, df.columns != \"Nu_pz\"].to_numpy()\n",
    "        # scaler = preprocessing.StandardScaler().fit(x)\n",
    "        # x = scaler.transform(x)\n",
    "        # df[df.columns[:-1]] = x\n",
    "        \n",
    "        self.data = torch.from_numpy(df.values).float()\n",
    " \n",
    "     def __getitem__(self, idx):\n",
    "         return self.data[idx]\n",
    " \n",
    "     def __len__(self):\n",
    "         return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLNuModel(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_fn):\n",
    "    \n",
    "    n_features = model.n_features\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        X = batch[:, :n_features]\n",
    "        Y = batch[:, n_features:]\n",
    "\n",
    "        out = model(X)\n",
    "        loss = loss_fn(out, Y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_split = [0.9, 0.05, 0.05], batch_size=8192, n_epochs=100, learning_rate=1e-3):\n",
    "    dataset = WLNuDataset()\n",
    "    n_features = len(dataset[0]) - 1 # last two columns are targets: N_z\n",
    "    \n",
    "    print(f\"# of input features: {n_features}\")\n",
    "    print(f\"Total number of samples: {len(dataset)}\")\n",
    "    \n",
    "    data_split = [int(i * len(dataset)) for i in data_split]\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(dataset, lengths=data_split)\n",
    "    \n",
    "    print(f\"Train set samples: {len(train_set)}\")\n",
    "    print(f\"Val set samples: {len(val_set)}\")\n",
    "    print(f\"Test set samples: {len(test_set)}\")\n",
    "    \n",
    "    train_loader, val_loader, test_loader = [DataLoader(i, batch_size=batch_size, shuffle=True) \n",
    "                                             for i in (train_set, val_set, test_set)]\n",
    "    \n",
    "    model = WLNuModel(n_features)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f\"\\n{'='*30}\\n\")\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print(f\"Epoch {epoch}\")\n",
    "        \n",
    "        train_loss = 0\n",
    "        \n",
    "        for train_batch in train_loader:\n",
    "            X = train_batch[:, :n_features]\n",
    "            Y = train_batch[:, n_features:]\n",
    "            \n",
    "            out = model(X)\n",
    "            loss = loss_fn(out, Y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad() # clear previous gradients\n",
    "            loss.backward() # compute gradients\n",
    "            \n",
    "            optimizer.step() # update weights using computed gradients\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Train loss: {train_loss}\")\n",
    "        \n",
    "        val_loss = evaluate(model, val_loader, loss_fn)\n",
    "        print(f\"Val loss: {val_loss}\")\n",
    "        \n",
    "        print(f\"\\n{'='*30}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input features: 5\n",
      "Total number of samples: 100000\n",
      "Train set samples: 90000\n",
      "Val set samples: 5000\n",
      "Test set samples: 5000\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 0\n",
      "Train loss: 1147.2495006214488\n",
      "Val loss: 1148.6844482421875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 1\n",
      "Train loss: 1125.5441450639205\n",
      "Val loss: 1122.2547607421875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 2\n",
      "Train loss: 1099.9109885475852\n",
      "Val loss: 1103.3150634765625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 3\n",
      "Train loss: 1067.02734375\n",
      "Val loss: 1087.123046875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 4\n",
      "Train loss: 1042.6960282759233\n",
      "Val loss: 1058.099365234375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 5\n",
      "Train loss: 1028.446660822088\n",
      "Val loss: 1048.319580078125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 6\n",
      "Train loss: 1008.5561689897017\n",
      "Val loss: 1032.2677001953125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 7\n",
      "Train loss: 986.9375554865056\n",
      "Val loss: 1014.1881713867188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 8\n",
      "Train loss: 969.2565585049716\n",
      "Val loss: 991.2457275390625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 9\n",
      "Train loss: 970.3288518732244\n",
      "Val loss: 1011.42041015625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 10\n",
      "Train loss: 955.773476340554\n",
      "Val loss: 981.4016723632812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 11\n",
      "Train loss: 952.3160677823154\n",
      "Val loss: 988.5363159179688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 12\n",
      "Train loss: 941.9237337979404\n",
      "Val loss: 984.7877807617188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 13\n",
      "Train loss: 930.785888671875\n",
      "Val loss: 975.976806640625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 14\n",
      "Train loss: 929.9947343306108\n",
      "Val loss: 972.5317993164062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 15\n",
      "Train loss: 925.2009776722301\n",
      "Val loss: 967.3010864257812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 16\n",
      "Train loss: 921.5809603604404\n",
      "Val loss: 968.5894165039062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 17\n",
      "Train loss: 916.4812954989346\n",
      "Val loss: 965.590087890625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 18\n",
      "Train loss: 913.5737804066051\n",
      "Val loss: 958.8309936523438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 19\n",
      "Train loss: 910.8061356977983\n",
      "Val loss: 962.5014038085938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 20\n",
      "Train loss: 906.5050159801136\n",
      "Val loss: 957.6306762695312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 21\n",
      "Train loss: 907.4928200461648\n",
      "Val loss: 959.5606079101562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 22\n",
      "Train loss: 903.2756569602273\n",
      "Val loss: 958.2991943359375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 23\n",
      "Train loss: 899.3771306818181\n",
      "Val loss: 956.1422729492188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 24\n",
      "Train loss: 897.9632679332386\n",
      "Val loss: 952.3909912109375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 25\n",
      "Train loss: 897.9498790394176\n",
      "Val loss: 950.6842041015625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 26\n",
      "Train loss: 904.477783203125\n",
      "Val loss: 963.7532958984375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 27\n",
      "Train loss: 900.1895973899148\n",
      "Val loss: 947.3411865234375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 28\n",
      "Train loss: 895.5076682350852\n",
      "Val loss: 949.3106079101562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 29\n",
      "Train loss: 893.3999911221591\n",
      "Val loss: 954.0703125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 30\n",
      "Train loss: 891.4752419211648\n",
      "Val loss: 944.0368041992188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 31\n",
      "Train loss: 895.5925237482244\n",
      "Val loss: 948.1663208007812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 32\n",
      "Train loss: 897.2783203125\n",
      "Val loss: 953.18310546875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 33\n",
      "Train loss: 895.2139393199574\n",
      "Val loss: 943.8544921875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 34\n",
      "Train loss: 887.6122103604404\n",
      "Val loss: 942.5194702148438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 35\n",
      "Train loss: 891.2562533291904\n",
      "Val loss: 949.6021118164062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 36\n",
      "Train loss: 886.2273226651279\n",
      "Val loss: 936.0596923828125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 37\n",
      "Train loss: 883.2714566317471\n",
      "Val loss: 938.2191772460938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 38\n",
      "Train loss: 876.4902010830966\n",
      "Val loss: 931.5982055664062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 39\n",
      "Train loss: 873.1789828213779\n",
      "Val loss: 927.5742797851562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 40\n",
      "Train loss: 870.7325494939631\n",
      "Val loss: 931.3690185546875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 41\n",
      "Train loss: 869.2249533913352\n",
      "Val loss: 933.4495239257812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 42\n",
      "Train loss: 872.9446133700284\n",
      "Val loss: 926.6505737304688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 43\n",
      "Train loss: 869.8203235973011\n",
      "Val loss: 922.243408203125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 44\n",
      "Train loss: 868.3155018199574\n",
      "Val loss: 920.600830078125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 45\n",
      "Train loss: 878.1192904385654\n",
      "Val loss: 914.3944091796875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 46\n",
      "Train loss: 859.9902010830966\n",
      "Val loss: 919.7742919921875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 47\n",
      "Train loss: 857.0325705788352\n",
      "Val loss: 917.1657104492188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 48\n",
      "Train loss: 850.1951182972301\n",
      "Val loss: 907.0620727539062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 49\n",
      "Train loss: 841.7743141867898\n",
      "Val loss: 895.3485717773438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 50\n",
      "Train loss: 837.2660189541904\n",
      "Val loss: 890.8021850585938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 51\n",
      "Train loss: 828.7000510475852\n",
      "Val loss: 883.7048950195312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 52\n",
      "Train loss: 822.8272760564631\n",
      "Val loss: 874.3215942382812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 53\n",
      "Train loss: 824.9403852982955\n",
      "Val loss: 880.9533081054688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 54\n",
      "Train loss: 828.3315707120029\n",
      "Val loss: 892.4030151367188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 55\n",
      "Train loss: 820.6497969193892\n",
      "Val loss: 862.031005859375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 56\n",
      "Train loss: 810.4587291370739\n",
      "Val loss: 855.9992065429688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 57\n",
      "Train loss: 800.3847767223011\n",
      "Val loss: 857.2343139648438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 58\n",
      "Train loss: 793.8289018110795\n",
      "Val loss: 877.2822265625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 59\n",
      "Train loss: 791.0687588778409\n",
      "Val loss: 834.2385864257812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 60\n",
      "Train loss: 775.778564453125\n",
      "Val loss: 819.0091552734375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 61\n",
      "Train loss: 760.7267567027699\n",
      "Val loss: 804.0820922851562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 62\n",
      "Train loss: 750.6235462535511\n",
      "Val loss: 798.1646728515625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 63\n",
      "Train loss: 741.6122824928977\n",
      "Val loss: 794.1973266601562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 64\n",
      "Train loss: 750.6483764648438\n",
      "Val loss: 837.869873046875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 65\n",
      "Train loss: 758.5928011807529\n",
      "Val loss: 815.6641235351562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 66\n",
      "Train loss: 742.7751353870739\n",
      "Val loss: 776.9866943359375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 67\n",
      "Train loss: 713.5548428622159\n",
      "Val loss: 759.365966796875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 68\n",
      "Train loss: 704.8409534801136\n",
      "Val loss: 745.5281372070312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 69\n",
      "Train loss: 700.0174782492898\n",
      "Val loss: 741.2017822265625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 70\n",
      "Train loss: 691.6732177734375\n",
      "Val loss: 727.007080078125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 71\n",
      "Train loss: 680.4528198242188\n",
      "Val loss: 715.8773193359375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 72\n",
      "Train loss: 670.0542158647017\n",
      "Val loss: 711.07177734375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 73\n",
      "Train loss: 661.3241577148438\n",
      "Val loss: 706.1599731445312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 74\n",
      "Train loss: 667.801435990767\n",
      "Val loss: 752.713623046875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 75\n",
      "Train loss: 671.6377563476562\n",
      "Val loss: 719.9840698242188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 76\n",
      "Train loss: 676.002685546875\n",
      "Val loss: 719.5450439453125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 77\n",
      "Train loss: 659.6875277432529\n",
      "Val loss: 709.2357788085938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 78\n",
      "Train loss: 642.428788618608\n",
      "Val loss: 686.0911254882812\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 79\n",
      "Train loss: 632.0070578835227\n",
      "Val loss: 677.966064453125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 80\n",
      "Train loss: 627.3778353604404\n",
      "Val loss: 677.0391235351562\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 623.1235850941051\n",
      "Val loss: 673.373046875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 82\n",
      "Train loss: 620.037070534446\n",
      "Val loss: 668.2620849609375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 83\n",
      "Train loss: 622.7220348011364\n",
      "Val loss: 679.9517211914062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 84\n",
      "Train loss: 628.1897250088779\n",
      "Val loss: 694.8091430664062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 85\n",
      "Train loss: 643.0382135564631\n",
      "Val loss: 697.4771118164062\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 86\n",
      "Train loss: 626.3065352006392\n",
      "Val loss: 660.7698974609375\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 87\n",
      "Train loss: 610.8686079545455\n",
      "Val loss: 662.1757202148438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 88\n",
      "Train loss: 603.3119451349431\n",
      "Val loss: 649.73388671875\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 89\n",
      "Train loss: 598.7633167613636\n",
      "Val loss: 651.1747436523438\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 90\n",
      "Train loss: 599.2256469726562\n",
      "Val loss: 656.9910278320312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 91\n",
      "Train loss: 600.8044544566761\n",
      "Val loss: 653.5519409179688\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 92\n",
      "Train loss: 602.8539373224431\n",
      "Val loss: 656.1383056640625\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 93\n",
      "Train loss: 611.362138227983\n",
      "Val loss: 680.7377319335938\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 94\n",
      "Train loss: 598.8529829545455\n",
      "Val loss: 652.8258666992188\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 95\n",
      "Train loss: 592.238603071733\n",
      "Val loss: 648.6602783203125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 96\n",
      "Train loss: 587.3768587979404\n",
      "Val loss: 639.8377075195312\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 97\n",
      "Train loss: 580.8802490234375\n",
      "Val loss: 640.8125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 98\n",
      "Train loss: 581.5414817116477\n",
      "Val loss: 645.544189453125\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 99\n",
      "Train loss: 584.8702670010654\n",
      "Val loss: 655.2645263671875\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}